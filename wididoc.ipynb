{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a5c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-01. Language Model\n",
    "# NOTE : What can we do using Language Model?\n",
    "# NOTE : Why infer next word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db6c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.02. SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebfd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\none-line summary : Considering certain count of N words to expect next word more accuratly.\\nconnection or question : if \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.03. N-gram Language Model\n",
    "\"\"\"\n",
    "one-line summary : Considering certain count of N words to expect next word more accuratly.\n",
    "connection or question : I can extract better 10 social dimensions from text. if precedure word from the word representing 10 social dimensions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6d16fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\none-line summary : Predicting the next word in Korean is challenging for Language Models dut to its flexible word order, agglutinative nature, and inconsistent spacing.\\nconnection or question : How can we improve prediction accuracy for Korean?\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.04. Language Model in Korean\n",
    "\"\"\"\n",
    "one-line summary : Predicting the next word in Korean is challenging for Language Models dut to its flexible word order, agglutinative nature, and inconsistent spacing.\n",
    "connection or question : How can we improve prediction accuracy for Korean?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7ffc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\none-line summary : Perplexity is a metric used to measure a language model's accuracy, indicating how uncertain the model is about its predictions.\\nconnection or quesion : Can Perplexity be used to measure a model's performance across the 10 social dimensions it predicted?\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.05. Perplecity\n",
    "\"\"\"\n",
    "one-line summary : Perplexity is a metric used to measure a language model's accuracy, indicating how uncertain the model is about its predictions.\n",
    "connection or quesion : Can Perplexity be used to measure a model's performance across the 10 social dimensions it predicted?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b7de55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\none-line summary : Words representations can broadly be divided into two categories: local(or discrete) representations and continuous representations. \\nconnection or question : Would continuous representations be more suitable for predicting the 10 social dimensions?\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.01. Various Ways to represent words\n",
    "\"\"\"\n",
    "one-line summary : Words representations can broadly be divided into two categories: local(or discrete) representations and continuous representations. \n",
    "connection or question : Would continuous representations be more suitable for predicting the 10 social dimensions?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a73cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\none-line summary : BoW is a simple text representation model that disregards word order and focuses soley on the frequency of words within a document.\\nconnection or question : Can I analyze sentences  categorized by 10 social dimensions using the BoW model?\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.02. Bag of Words (BoW)\n",
    "\"\"\"\n",
    "one-line summary : BoW is a simple text representation model that disregards word order and focuses soley on the frequency of words within a document.\n",
    "connection or question : Can I analyze sentences  categorized by 10 social dimensions using the BoW model?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdfb333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW function\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def build_bag_of_words(document):\n",
    "  # 온점 제거 및 형태소 분석\n",
    "  document = document.replace('.', '')\n",
    "  tokenized_document = okt.morphs(document)\n",
    "\n",
    "  word_to_index = {}\n",
    "  bow = []\n",
    "\n",
    "  for word in tokenized_document:  \n",
    "    if word not in word_to_index.keys():\n",
    "      word_to_index[word] = len(word_to_index)  \n",
    "      # BoW에 전부 기본값 1을 넣는다.\n",
    "      bow.insert(len(word_to_index) - 1, 1)\n",
    "    else:\n",
    "      # 재등장하는 단어의 인덱스\n",
    "      index = word_to_index.get(word)\n",
    "      # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더한다.\n",
    "      bow[index] = bow[index] + 1\n",
    "\n",
    "  return word_to_index, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad7e77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
    "vocab, bow = build_bag_of_words(doc1)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb4afad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "bag of words vector : [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc2 = '소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\n",
    "\n",
    "vocab, bow = build_bag_of_words(doc2)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d49977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "bag of words vector : [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc3 = doc1 + ' ' + doc2\n",
    "vocab, bow = build_bag_of_words(doc3)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952a4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 2 1 2 1]]\n",
      "vocabulary : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print('bag of words vector :', vector.fit_transform(corpus).toarray()) \n",
    "\n",
    "# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\n",
    "print('vocabulary :',vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eff5893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6b7d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1]]\n",
      "vocabulary : {'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51e7189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words=stop_words)\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray()) \n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50f2f341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\none-line summary : DTM is created by compliling individual BoW representations into a single matrix. However, it often faces two main limitations : the sparsity problem and its sole reliance on word frequency.\\nconnnection or question : Can I use a DTM to alnalyze sentences, especially those containing specific social dimensions?\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.03. Document-Term Matrix(DTM)\n",
    "\"\"\"\n",
    "one-line summary : DTM is created by compliling individual BoW representations into a single matrix. However, it often faces two main limitations : the sparsity problem and its sole reliance on word frequency.\n",
    "connnection or question : Can I use a DTM to alnalyze sentences, especially those containing specific social dimensions?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "887118d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\none-line summary : TF-IDF is a metric that assigns a weight to each word, allowing us to consider more information than with a simple DTM.\\nconnection or question : Can I use TF-IDF to identify the most important words within the 10 social dimensions?\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.04. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\"\"\"\n",
    "one-line summary : TF-IDF is a metric that assigns a weight to each word, allowing us to consider more information than with a simple DTM.\n",
    "connection or question : Can I use TF-IDF to identify the most important words within the 10 social dimensions?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73fa2fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # 데이터프레임 사용을 위해\n",
    "from math import log # IDF 계산을 위해\n",
    "\n",
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "] \n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47fcafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 문서의 수\n",
    "N = len(docs) \n",
    "\n",
    "def tf(t, d):\n",
    "  return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "  df = 0\n",
    "  for doc in docs:\n",
    "    df += t in doc\n",
    "  return log(N/(df+1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "  return tf(t,d)* idf(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e48f41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "# 각 문서에 대해서 아래 연산을 반복\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = docs[i]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tf(t, d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns = vocab)\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f50ce0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index=vocab, columns=[\"IDF\"])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdd76601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         저는       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = docs[i]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tfidf(t,d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b829f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어와 맵핑된 인덱스 출력\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10509a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c94982c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\none-line summary : Cosine Similarity  a metric used to measure the imilarity between documents.\\nconnection or question : Can I use Cosine Similaryity to measure the similarity between representations of certain social dimensions?\\nnotation \\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 05.01 Cosine Similarity\n",
    "\"\"\"\n",
    "one-line summary : Cosine Similarity  a metric used to measure the imilarity between documents.\n",
    "connection or question : Can I use Cosine Similaryity to measure the similarity between representations of certain social dimensions?\n",
    "notation : Cosine Similarity helps when the lengths of documents are different.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69309bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1과 문서2의 유사도 : 0.6666666666666667\n",
      "문서 1과 문서3의 유사도 : 0.6666666666666667\n",
      "문서 2와 문서3의 유사도 : 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(A, B):\n",
    "  return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "doc1 = np.array([0,1,1,1])\n",
    "doc2 = np.array([1,0,1,1])\n",
    "doc3 = np.array([2,0,2,2])\n",
    "\n",
    "print('문서 1과 문서2의 유사도 :',cos_sim(doc1, doc2))\n",
    "print('문서 1과 문서3의 유사도 :',cos_sim(doc1, doc3))\n",
    "print('문서 2와 문서3의 유사도 :',cos_sim(doc2, doc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eccff12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\none-line summary : Euclidean distance is a metric that calculates the straight-line distance between two points.\\nJaccard similarity is a metric that measures the ratio of the size of the intersection to the size of the union of two sets.\\n\\nconnection or question : Is jaccard similarity more efficient than cosine similarity?\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.02. Various Similarity Methods\n",
    "\"\"\"\n",
    "one-line summary : Euclidean distance is a metric that calculates the straight-line distance between two points.\n",
    "Jaccard similarity is a metric that measures the ratio of the size of the intersection to the size of the union of two sets.\n",
    "\n",
    "connection or question : Is jaccard similarity more efficient than cosine similarity?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "710ee9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서1과 문서Q의 거리 : 2.23606797749979\n",
      "문서2과 문서Q의 거리 : 3.1622776601683795\n",
      "문서3과 문서Q의 거리 : 2.449489742783178\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dist(x,y):   \n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "doc1 = np.array((2,3,0,1))\n",
    "doc2 = np.array((1,2,3,1))\n",
    "doc3 = np.array((2,1,2,2))\n",
    "docQ = np.array((1,1,0,1))\n",
    "\n",
    "print('문서1과 문서Q의 거리 :',dist(doc1,docQ))\n",
    "print('문서2과 문서Q의 거리 :',dist(doc2,docQ))\n",
    "print('문서3과 문서Q의 거리 :',dist(doc3,docQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9805705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서1 : ['apple', 'banana', 'everyone', 'like', 'likey', 'watch', 'card', 'holder']\n",
      "문서2 : ['apple', 'banana', 'coupon', 'passport', 'love', 'you']\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"apple banana everyone like likey watch card holder\"\n",
    "doc2 = \"apple banana coupon passport love you\"\n",
    "\n",
    "# 토큰화\n",
    "tokenized_doc1 = doc1.split()\n",
    "tokenized_doc2 = doc2.split()\n",
    "\n",
    "print('문서1 :',tokenized_doc1)\n",
    "print('문서2 :',tokenized_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce6ab8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서1과 문서2의 합집합 : {'likey', 'watch', 'passport', 'coupon', 'banana', 'card', 'love', 'everyone', 'apple', 'you', 'like', 'holder'}\n"
     ]
    }
   ],
   "source": [
    "union = set(tokenized_doc1).union(set(tokenized_doc2))\n",
    "print('문서1과 문서2의 합집합 :',union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c2f0394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서1과 문서2의 교집합 : {'banana', 'apple'}\n"
     ]
    }
   ],
   "source": [
    "intersection = set(tokenized_doc1).intersection(set(tokenized_doc2))\n",
    "print('문서1과 문서2의 교집합 :',intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1273b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자카드 유사도 : 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "print('자카드 유사도 :',len(intersection)/len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44caa2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
